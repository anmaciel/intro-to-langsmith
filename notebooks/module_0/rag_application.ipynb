{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple RAG](../../images/simple_rag.png)\n",
    "\n",
    "In this notebook, we're going to set up a simple RAG application that we'll be using as we learn more about LangSmith.\n",
    "\n",
    "RAG (Retrieval Augmented Generation) is a popular technique for providing LLMs with relevant documents that will enable them to better answer questions from users. \n",
    "\n",
    "In our case, we are going to index some LangSmith documentation!\n",
    "\n",
    "LangSmith makes it easy to trace any LLM application, no LangChain required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Make sure you set your environment variables, including your Google API key."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# You can set them inline!\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat, get_gemini_model_name\n\nfrom langsmith import traceable\nfrom typing import List\nimport nest_asyncio\nfrom utils import get_vector_db_retriever\n\nMODEL_PROVIDER = \"google\"\nMODEL_NAME = \"gemini-1.5-flash\"\nAPP_VERSION = 1.0\nRAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the latest question in the conversation. \nIf you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\n\"\"\"\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\n\"\"\"\nretrieve_documents\n- Returns documents fetched from a vectorstore based on the user's question\n\"\"\"\n@traceable(run_type=\"chain\")\ndef retrieve_documents(question: str):\n    return retriever.invoke(question)\n\n\"\"\"\ngenerate_response\n- Calls `call_gemini` to generate a model response after formatting inputs\n\"\"\"\n@traceable(run_type=\"chain\")\ndef generate_response(question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n        }\n    ]\n    return call_gemini(messages)\n\n\"\"\"\ncall_gemini\n- Returns the chat completion output from Google Gemini\n\"\"\"\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": MODEL_PROVIDER,\n        \"ls_model_name\": MODEL_NAME\n    }\n)\ndef call_gemini(messages: List[dict], temperature: float = 0.0) -> str:\n    return call_gemini_chat(MODEL_NAME, messages, temperature)\n\n\"\"\"\nlangsmith_rag\n- Calls `retrieve_documents` to fetch documents\n- Calls `generate_response` to generate a response based on the fetched documents\n- Returns the model response\n\"\"\"\n@traceable(run_type=\"chain\")\ndef langsmith_rag(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should take a little less than a minute. We are indexing and storing LangSmith documentation in a SKLearn vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is LangSmith used for?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"website\": \"www.google.com\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look in LangSmith!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}