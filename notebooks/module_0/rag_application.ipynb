{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Aplicação RAG"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "![Simple RAG](../../images/simple_rag.png)\n\nNeste notebook, vamos configurar uma aplicação RAG simples que usaremos para aprender mais sobre o LangSmith.\n\nRAG (Retrieval Augmented Generation) é uma técnica popular para fornecer aos LLMs documentos relevantes que permitirão que respondam melhor às perguntas dos usuários.\n\nNo nosso caso, vamos indexar documentação do LangSmith!\n\nO LangSmith facilita o rastreamento de qualquer aplicação LLM, sem necessidade de LangChain!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configuração"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Certifique-se de definir suas variáveis de ambiente, incluindo sua chave de API do Google."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Você pode defini-las diretamente!\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Aplicação RAG Simples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat, get_gemini_model_name\n\nfrom langsmith import traceable\nfrom typing import List\nimport nest_asyncio\nfrom utils import get_vector_db_retriever\n\nMODEL_PROVIDER = \"google\"\nMODEL_NAME = \"gemini-1.5-flash\"\nAPP_VERSION = 1.0\nRAG_SYSTEM_PROMPT = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\nSe você não souber a resposta, apenas diga que não sabe.\nUse no máximo três frases e mantenha a resposta concisa.\n\"\"\"\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\n\"\"\"\nretrieve_documents\n- Retorna documentos buscados de um armazenamento vetorial baseado na pergunta do usuário\n\"\"\"\n@traceable(run_type=\"chain\")\ndef retrieve_documents(question: str):\n    return retriever.invoke(question)\n\n\"\"\"\ngenerate_response\n- Chama `call_gemini` para gerar uma resposta do modelo após formatar entradas\n\"\"\"\n@traceable(run_type=\"chain\")\ndef generate_response(question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Contexto: {formatted_docs} \\n\\n Pergunta: {question}\"\n        }\n    ]\n    return call_gemini(messages)\n\n\"\"\"\ncall_gemini\n- Retorna a saída de conclusão de chat do Google Gemini\n\"\"\"\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": MODEL_PROVIDER,\n        \"ls_model_name\": MODEL_NAME\n    }\n)\ndef call_gemini(messages: List[dict], temperature: float = 0.0) -> str:\n    return call_gemini_chat(MODEL_NAME, messages, temperature)\n\n\"\"\"\nlangsmith_rag\n- Chama `retrieve_documents` para buscar documentos\n- Chama `generate_response` para gerar uma resposta baseada nos documentos buscados\n- Retorna a resposta do modelo\n\"\"\"\n@traceable(run_type=\"chain\")\ndef langsmith_rag(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Isso deve levar um pouco menos de um minuto. Estamos indexando e armazenando documentação do LangSmith em um banco de dados vetorial SKLearn."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Para que é usado o LangSmith?\"\nai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"website\": \"www.google.com\"}})\nprint(ai_answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Vamos dar uma olhada no LangSmith!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}