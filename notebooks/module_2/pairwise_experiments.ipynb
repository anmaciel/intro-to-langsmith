{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experimentos Pareados"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configuração"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Você pode defini-las diretamente\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tarefa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Vamos configurar uma nova tarefa! Aqui, temos um vendedor chamado Bob. Bob tem muitos negócios, então ele quer resumir o que aconteceu nesses negócios baseado em algumas transcrições de reuniões.\n\nBob está iterando em alguns prompts diferentes, que lhe darão transcrições legais e concisas para seus negócios.\n\nBob curou um dataset de suas transcrições de negócios, vamos carregá-lo. Você pode dar uma olhada no dataset também se estiver curioso! Note que este não é um dataset dourado, não há saída de referência aqui."
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\n",
    "  \"https://smith.langchain.com/public/9078d2f1-7bef-4ba7-b795-210a17682ef9/d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Experimentos"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Agora, vamos executar alguns experimentos neste dataset usando dois prompts diferentes. Vamos adicionar um avaliador que tenta pontuar quão bons são nossos resumos!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pydantic import BaseModel, Field\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\n\nSUMMARIZATION_SYSTEM_PROMPT = \"\"\"Você é um juiz, visando pontuar quão bem um resumo resume o conteúdo de uma transcrição\"\"\"\n\nSUMMARIZATION_HUMAN_PROMPT = \"\"\"\n[A Transcrição da Reunião] {transcript}\n[O Início do Resumo] {summary} [O Fim do Resumo]\"\"\"\n\nclass SummarizationScore(BaseModel):\n    score: int = Field(description=\"\"\"Uma pontuação de 1-5 classificando quão bom é o resumo para a transcrição fornecida, com 1 sendo um resumo ruim, e 5 sendo um ótimo resumo\"\"\")\n    \ndef summary_score_evaluator(inputs: dict, outputs: dict) -> list:\n    messages = [\n        {   \n            \"role\": \"system\",\n            \"content\": SUMMARIZATION_SYSTEM_PROMPT,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n                transcript=inputs[\"transcript\"],\n                summary=outputs.get(\"output\", \"N/A\"),\n            )}\n    ]\n    \n    response = call_gemini_chat(\"gemini-1.5-flash\", messages, 0.0)\n    \n    # Extrair pontuação da resposta (simplificado)\n    try:\n        import re\n        score_match = re.search(r'\\b([1-5])\\b', response)\n        if score_match:\n            score = int(score_match.group(1))\n        else:\n            score = 3  # pontuação padrão\n    except:\n        score = 3\n    \n    return {\"key\": \"summary_score\", \"score\": score}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Primeiro, vamos executar nosso experimento com uma boa versão do nosso prompt!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt Um: Bom Prompt!\ndef good_summarizer(inputs: dict):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": f\"Resuma concisamente esta reunião em 3 frases. Certifique-se de incluir todos os eventos importantes. Reunião: {inputs['transcript']}\"\n        }\n    ]\n    \n    response = call_gemini_chat(\"gemini-1.5-flash\", messages, 0.0)\n    return response\n\nclient.evaluate(\n    good_summarizer,\n    data=dataset,\n    evaluators=[summary_score_evaluator],\n    experiment_prefix=\"Bom Resumidor\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Agora, vamos executar um experimento com uma versão pior do nosso prompt, para destacar a diferença."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prompt Dois: Prompt Pior!\ndef bad_summarizer(inputs: dict):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": f\"Resuma isso em uma frase. {inputs['transcript']}\"\n        }\n    ]\n    \n    response = call_gemini_chat(\"gemini-1.5-flash\", messages, 0.0)\n    return response\n\nclient.evaluate(\n    bad_summarizer,\n    data=dataset,\n    evaluators=[summary_score_evaluator],\n    experiment_prefix=\"Resumidor Ruim\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Experimento Pareado"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Vamos definir uma função que comparará nossos dois experimentos. Estes são os campos aos quais as funções de avaliador pareado têm acesso:\n- `inputs: dict`: Um dicionário das entradas correspondentes a um único exemplo em um dataset.\n- `outputs: list[dict]`: Uma lista das saídas dict produzidas por cada experimento nas entradas dadas.\n- `reference_outputs: dict`: Um dicionário das saídas de referência associadas ao exemplo, se disponível.\n- `runs: list[Run]`: Uma lista dos objetos Run completos gerados pelos experimentos no exemplo dado. Use isso se precisar de acesso a passos intermediários ou metadados sobre cada execução.\n- `example: Example`: O Example completo do dataset, incluindo as entradas do exemplo, saídas (se disponível) e metadata (se disponível)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Primeiro, vamos dar ao nosso LLM-as-Judge algumas instruções. No nosso caso, vamos usar diretamente LLM-as-judge para classificar qual dos resumidores é o mais útil.\n\nPode ser difícil classificar nossos resumidores sem uma referência de verdade fundamental, mas aqui, comparar diferentes prompts cara a cara nos dará uma noção de qual é melhor!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "JUDGE_SYSTEM_PROMPT = \"\"\"\nPor favor, aja como um juiz imparcial e avalie a qualidade dos resumos fornecidos por dois resumidores de IA para a transcrição da reunião abaixo.\nSua avaliação deve considerar fatores como a utilidade, relevância, precisão, profundidade, criatividade e nível de detalhe de seus resumos.\nComece sua avaliação comparando os dois resumos e forneça uma breve explicação.\nEvite quaisquer vieses de posição e certifique-se de que a ordem na qual as respostas foram apresentadas não influencie sua decisão.\nNão favoreça certos nomes dos assistentes.\nSeja o mais objetivo possível. \"\"\"\n\nJUDGE_HUMAN_PROMPT = \"\"\"\n[A Transcrição da Reunião] {transcript}\n\n[O Início do Resumo do Assistente A] {answer_a} [O Fim do Resumo do Assistente A]\n\n[O Início do Resumo do Assistente B] {answer_b} [O Fim do Resumo do Assistente B]\"\"\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Nossa função receberá um dicionário `inputs`, e uma lista de dicionários `outputs` para os diferentes experimentos que queremos comparar."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pydantic import BaseModel, Field\n\nclass Preference(BaseModel):\n    preference: int = Field(description=\"\"\"1 se a resposta do Assistente A for melhor baseada nos fatores acima.\n2 se a resposta do Assistente B for melhor baseada nos fatores acima.\nSaída 0 se for um empate.\"\"\")\n    \ndef ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n    messages = [\n        {   \n            \"role\": \"system\",\n            \"content\": JUDGE_SYSTEM_PROMPT,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": JUDGE_HUMAN_PROMPT.format(\n                transcript=inputs[\"transcript\"],\n                answer_a=outputs[0].get(\"output\", \"N/A\"),\n                answer_b=outputs[1].get(\"output\", \"N/A\")\n            )}\n    ]\n    \n    response = call_gemini_chat(\"gemini-1.5-flash\", messages, 0.0)\n    \n    # Extrair preferência da resposta (simplificado)\n    try:\n        import re\n        if \"assistente a\" in response.lower() and \"melhor\" in response.lower():\n            preference_score = 1\n        elif \"assistente b\" in response.lower() and \"melhor\" in response.lower():\n            preference_score = 2\n        else:\n            preference_score = 0\n    except:\n        preference_score = 0\n\n    if preference_score == 1:\n        scores = [1, 0]\n    elif preference_score == 2:\n        scores = [0, 1]\n    else:\n        scores = [0, 0]\n    return scores"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Agora vamos executar nosso experimento pareado com `evaluate()`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import evaluate\n\nevaluate(\n    (\"Bom Resumidor-bafea4ec\", \"Resumidor Ruim-06ff299d\"),  # TODO: Substitua pelos nomes/IDs dos seus experimentos\n    evaluators=[ranked_preference]\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}