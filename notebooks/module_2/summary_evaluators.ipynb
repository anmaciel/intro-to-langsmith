{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Avaliadores de Resumo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configuração"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Você pode defini-las diretamente\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tarefa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Nossa tarefa aqui é analisar a toxicidade de declarações aleatórias, classificando-as como `Tóxico` ou `Não tóxico`.\n\nDê uma olhada no nosso dataset!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\n",
    "    \"https://smith.langchain.com/public/89ef0d44-a252-4011-8bb8-6a114afc1522/d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Este é um classificador de toxicidade simples!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\nfrom pydantic import BaseModel, Field\n\nclass Toxicity(BaseModel):\n    toxicity: str = Field(description=\"\"\"'Tóxico' se a declaração for tóxica, 'Não tóxico' se a declaração não for tóxica.\"\"\")\n\ndef good_classifier(inputs: dict) -> dict:\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": f\"Esta é a declaração: {inputs['statement']}. Classifique como 'Tóxico' ou 'Não tóxico'.\"\n        }\n    ]\n    \n    response = call_gemini_chat(\"gemini-1.5-flash\", messages, 0.0)\n    \n    # Extrair classificação da resposta (simplificado)\n    if \"tóxico\" in response.lower() and \"não\" not in response.lower():\n        toxicity_score = \"Tóxico\"\n    else:\n        toxicity_score = \"Não tóxico\"\n    \n    return {\"class\": toxicity_score}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Avaliador de Resumo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Estes são os campos aos quais as funções de avaliador de resumo têm acesso:\n- `inputs: list[dict]`: Uma lista de entradas dos exemplos em nosso dataset\n- `outputs: list[dict]`: Uma lista das saídas dict produzidas ao executar nosso alvo sobre cada entrada\n- `reference_outputs: list[dict]`: Uma lista de reference_outputs dos exemplos em nosso dataset\n- `runs: list[Run]`: Uma lista dos objetos Run de executar nosso alvo sobre o dataset.\n- `examples: list[Example]`: Uma lista dos Examples completos do dataset, incluindo as entradas do exemplo, saídas (se disponível) e metadata (se disponível)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Agora vamos definir nosso avaliador de resumo! Aqui, vamos calcular o f1-score, que é uma combinação de precisão e recall.\n\nEste tipo de métrica só pode ser calculada sobre todos os exemplos em nosso experimento, então nosso avaliador recebe uma lista de saídas e uma lista de reference_outputs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def f1_score_summary_evaluator(outputs: list[dict], reference_outputs: list[dict]) -> dict:\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    for output_dict, reference_output_dict in zip(outputs, reference_outputs):\n        output = output_dict[\"class\"]\n        reference_output = reference_output_dict[\"class\"]\n        if output == \"Tóxico\" and reference_output == \"Tóxico\":\n            true_positives += 1\n        elif output == \"Tóxico\" and reference_output == \"Não tóxico\":\n            false_positives += 1\n        elif output == \"Não tóxico\" and reference_output == \"Tóxico\":\n            false_negatives += 1\n\n    if true_positives == 0:\n        return {\"key\": \"f1_score\", \"score\": 0.0}\n\n    precision = true_positives / (true_positives + false_positives)\n    recall = true_positives / (true_positives + false_negatives)\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return {\"key\": \"f1_score\", \"score\": f1_score}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Note que passamos `f1_score_summary_evaluator` como um avaliador de resumo!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = client.evaluate(\n    good_classifier,\n    data=dataset,\n    summary_evaluators=[f1_score_summary_evaluator],\n    experiment_prefix=\"Bom classificador\"\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}