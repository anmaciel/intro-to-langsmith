{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Avaliadores"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Em alto nível, um avaliador julga uma invocação de sua aplicação LLM contra um exemplo de referência, e retorna uma pontuação de avaliação.\n\nNos avaliadores do LangSmith, representamos este processo como uma função que recebe um Run (representando a invocação da aplicação LLM) e um Example (representando o ponto de dados a ser avaliado), e retorna Feedback (representando a pontuação do avaliador da invocação da aplicação LLM)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evaluator](../../images/evaluator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Aqui está um exemplo de um avaliador personalizado muito simples que compara a saída de um modelo com a saída esperada no dataset:"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def correct_label(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n",
    "  score = outputs.get(\"output\") == reference_outputs.get(\"label\")\n",
    "  return {\"score\": int(score), \"key\": \"correct_label\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Avaliação LLM-as-Judge"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Avaliadores LLM-as-judge usam LLMs para pontuar a saída do sistema. Para usá-los, você tipicamente codifica as regras/critérios de classificação no prompt do LLM. Eles podem ser livres de referência (ex., verificar se a saída do sistema contém conteúdo ofensivo ou adere a critérios específicos). Ou, eles podem comparar a saída da tarefa com uma referência (ex., verificar se a saída é factualmente precisa em relação à referência).\n\nAqui está um exemplo de como você pode definir um avaliador LLM-as-judge com saída estruturada"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Você pode defini-las diretamente\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\nfrom pydantic import BaseModel, Field\n\nclass Similarity_Score(BaseModel):\n    similarity_score: int = Field(description=\"Pontuação de similaridade semântica entre 1 e 10, onde 1 significa não relacionado e 10 significa idêntico.\")\n\n# NOTA: Este é nosso avaliador\ndef compare_semantic_similarity(inputs: dict, reference_outputs: dict, outputs: dict):\n    input_question = inputs[\"question\"]\n    reference_response = reference_outputs[\"output\"]\n    run_response = outputs[\"output\"]\n    \n    messages = [\n        {   \n            \"role\": \"system\",\n            \"content\": (\n                \"Você é um avaliador de similaridade semântica. Compare os significados de duas respostas a uma pergunta, \"\n                \"Resposta de Referência e Nova Resposta, onde a referência é a resposta correta, e estamos tentando julgar se a nova resposta é similar. \"\n                \"Forneça uma pontuação entre 1 e 10, onde 1 significa completamente não relacionado, e 10 significa idêntico em significado.\"\n            ),\n        },\n        {\"role\": \"user\", \"content\": f\"Pergunta: {input_question}\\n Resposta de Referência: {reference_response}\\n Resposta da Execução: {run_response}\"}\n    ]\n    \n    response = call_gemini_chat(\"gemini-1.5-flash\", messages, 0.0)\n    \n    # Extrair pontuação da resposta (simplificado)\n    try:\n        import re\n        score_match = re.search(r'\\b([1-9]|10)\\b', response)\n        if score_match:\n            score = int(score_match.group(1))\n        else:\n            score = 5  # pontuação padrão se não conseguir extrair\n    except:\n        score = 5\n    \n    return {\"score\": score, \"key\": \"similarity\"}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Vamos testar isso!\n\nNOTA: Propositalmente fizemos esta resposta errada, então esperamos ver uma pontuação baixa."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Do Exemplo do Dataset\ninputs = {\n  \"question\": \"O LangSmith está nativamente integrado com o LangChain?\"\n}\nreference_outputs = {\n  \"output\": \"Sim, o LangSmith está nativamente integrado com o LangChain, assim como com o LangGraph.\"\n}\n\n\n# Da Execução\noutputs = {\n  \"output\": \"Não, o LangSmith NÃO está integrado com o LangChain.\"\n}\n\nsimilarity_score = compare_semantic_similarity(inputs, reference_outputs, outputs)\nprint(f\"Pontuação de similaridade semântica: {similarity_score}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Você também pode definir avaliadores usando Run e Example diretamente!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith.schemas import Run, Example\n\ndef compare_semantic_similarity_v2(root_run: Run, example: Example):\n    input_question = example[\"inputs\"][\"question\"]\n    reference_response = example[\"outputs\"][\"output\"]\n    run_response = root_run[\"outputs\"][\"output\"]\n    \n    messages = [\n        {   \n            \"role\": \"system\",\n            \"content\": (\n                \"Você é um avaliador de similaridade semântica. Compare os significados de duas respostas a uma pergunta, \"\n                \"Resposta de Referência e Nova Resposta, onde a referência é a resposta correta, e estamos tentando julgar se a nova resposta é similar. \"\n                \"Forneça uma pontuação entre 1 e 10, onde 1 significa completamente não relacionado, e 10 significa idêntico em significado.\"\n            ),\n        },\n        {\"role\": \"user\", \"content\": f\"Pergunta: {input_question}\\n Resposta de Referência: {reference_response}\\n Resposta da Execução: {run_response}\"}\n    ]\n    \n    response = call_gemini_chat(\"gemini-1.5-flash\", messages, 0.0)\n    \n    # Extrair pontuação da resposta (simplificado)\n    try:\n        import re\n        score_match = re.search(r'\\b([1-9]|10)\\b', response)\n        if score_match:\n            score = int(score_match.group(1))\n        else:\n            score = 5  # pontuação padrão se não conseguir extrair\n    except:\n        score = 5\n\n    return {\"score\": score, \"key\": \"similarity\"}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sample_run = {\n  \"name\": \"Execução de Exemplo\",\n  \"inputs\": {\n    \"question\": \"O LangSmith está nativamente integrado com o LangChain?\"\n  },\n  \"outputs\": {\n    \"output\": \"Não, o LangSmith NÃO está integrado com o LangChain.\"\n  },\n  \"is_root\": True,\n  \"status\": \"success\",\n  \"extra\": {\n    \"metadata\": {\n      \"key\": \"value\"\n    }\n  }\n}\n\nsample_example = {\n  \"inputs\": {\n    \"question\": \"O LangSmith está nativamente integrado com o LangChain?\"\n  },\n  \"outputs\": {\n    \"output\": \"Sim, o LangSmith está nativamente integrado com o LangChain, assim como com o LangGraph.\"\n  },\n  \"metadata\": {\n    \"dataset_split\": [\n      \"AI generated\",\n      \"base\"\n    ]\n  }\n}\n\nsimilarity_score = compare_semantic_similarity_v2(sample_run, sample_example)\nprint(f\"Pontuação de similaridade semântica: {similarity_score}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}