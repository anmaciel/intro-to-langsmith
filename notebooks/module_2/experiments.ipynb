{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experimentos"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configuração"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Você pode defini-las diretamente\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Aqui está a Aplicação RAG com a qual temos trabalhado ao longo deste curso"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport tempfile\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders.sitemap import SitemapLoader\nfrom langchain_community.vectorstores import SKLearnVectorStore\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langsmith import traceable\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\nfrom typing import List\nimport nest_asyncio\n\n# TODO: Configure este modelo!\nMODEL_NAME = \"gemini-1.5-flash\"\nMODEL_PROVIDER = \"google\"\nAPP_VERSION = 1.0\nRAG_SYSTEM_PROMPT = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\nSe você não souber a resposta, apenas diga que não sabe.\nUse no máximo três frases e mantenha a resposta concisa.\n\"\"\"\n\ndef get_vector_db_retriever():\n    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n    embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n\n    # Se o armazenamento vetorial existir, então carregá-lo\n    if os.path.exists(persist_path):\n        vectorstore = SKLearnVectorStore(\n            embedding=embd,\n            persist_path=persist_path,\n            serializer=\"parquet\"\n        )\n        return vectorstore.as_retriever(lambda_mult=0)\n\n    # Caso contrário, indexar documentos LangSmith e criar novo armazenamento vetorial\n    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n    ls_docs = ls_docs_sitemap_loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=500, chunk_overlap=0\n    )\n    doc_splits = text_splitter.split_documents(ls_docs)\n\n    vectorstore = SKLearnVectorStore.from_documents(\n        documents=doc_splits,\n        embedding=embd,\n        persist_path=persist_path,\n        serializer=\"parquet\"\n    )\n    vectorstore.persist()\n    return vectorstore.as_retriever(lambda_mult=0)\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\n\"\"\"\nretrieve_documents\n- Retorna documentos buscados de um armazenamento vetorial baseado na pergunta do usuário\n\"\"\"\n@traceable(run_type=\"chain\")\ndef retrieve_documents(question: str):\n    return retriever.invoke(question)\n\n\"\"\"\ngenerate_response\n- Chama `call_openai` para gerar uma resposta do modelo após formatar entradas\n\"\"\"\n@traceable(run_type=\"chain\")\ndef generate_response(question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n        }\n    ]\n    return call_gemini(messages)\n\n\"\"\"\ncall_gemini\n- Retorna a saída de conclusão de chat do Google Gemini\n\"\"\"\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": MODEL_PROVIDER,\n        \"ls_model_name\": MODEL_NAME\n    }\n)\ndef call_gemini(messages: List[dict]) -> str:\n    return call_gemini_chat(MODEL_NAME, messages, 0.0)\n\n\"\"\"\nlangsmith_rag\n- Chama `retrieve_documents` para buscar documentos\n- Chama `generate_response` para gerar uma resposta baseada nos documentos buscados\n- Retorna a resposta do modelo\n\"\"\"\n@traceable(run_type=\"chain\")\ndef langsmith_rag(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Experimento"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Aqui está um trecho de código que deve parecer similar ao que você vê no código inicial!\n\nExistem alguns componentes importantes aqui.\n\n1. Definimos um Avaliador\n2. Canalizamos nossos exemplos de dataset (dict) para o formato de entrada que nossa função `langsmith_rag` aceita (str) usando uma função de destino"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport tempfile\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders.sitemap import SitemapLoader\nfrom langchain_community.vectorstores import SKLearnVectorStore\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langsmith import traceable\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\nfrom typing import List\nimport nest_asyncio\n\n# TODO: Configure este modelo!\nMODEL_NAME = \"gemini-1.5-flash\"\nMODEL_PROVIDER = \"google\"\nAPP_VERSION = 1.0\nRAG_SYSTEM_PROMPT = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\nSe você não souber a resposta, apenas diga que não sabe.\nUse no máximo três frases e mantenha a resposta concisa.\n\"\"\"\n\ndef get_vector_db_retriever():\n    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n    embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n\n    # Se o armazenamento vetorial existir, então carregá-lo\n    if os.path.exists(persist_path):\n        vectorstore = SKLearnVectorStore(\n            embedding=embd,\n            persist_path=persist_path,\n            serializer=\"parquet\"\n        )\n        return vectorstore.as_retriever(lambda_mult=0)\n\n    # Caso contrário, indexar documentos LangSmith e criar novo armazenamento vetorial\n    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n    ls_docs = ls_docs_sitemap_loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=500, chunk_overlap=0\n    )\n    doc_splits = text_splitter.split_documents(ls_docs)\n\n    vectorstore = SKLearnVectorStore.from_documents(\n        documents=doc_splits,\n        embedding=embd,\n        persist_path=persist_path,\n        serializer=\"parquet\"\n    )\n    vectorstore.persist()\n    return vectorstore.as_retriever(lambda_mult=0)\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\n\"\"\"\nretrieve_documents\n- Retorna documentos buscados de um armazenamento vetorial baseado na pergunta do usuário\n\"\"\"\n@traceable(run_type=\"chain\")\ndef retrieve_documents(question: str):\n    return retriever.invoke(question)\n\n\"\"\"\ngenerate_response\n- Chama `call_gemini` para gerar uma resposta do modelo após formatar entradas\n\"\"\"\n@traceable(run_type=\"chain\")\ndef generate_response(question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n        }\n    ]\n    return call_gemini(messages)\n\n\"\"\"\ncall_gemini\n- Retorna a saída de conclusão de chat do Google Gemini\n\"\"\"\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": MODEL_PROVIDER,\n        \"ls_model_name\": MODEL_NAME\n    }\n)\ndef call_gemini(messages: List[dict]) -> str:\n    return call_gemini_chat(MODEL_NAME, messages, 0.0)\n\n\"\"\"\nlangsmith_rag\n- Chama `retrieve_documents` para buscar documentos\n- Chama `generate_response` para gerar uma resposta baseada nos documentos buscados\n- Retorna a resposta do modelo\n\"\"\"\n@traceable(run_type=\"chain\")\ndef langsmith_rag(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Modificando sua Aplicação"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Agora, vamos mudar nosso modelo para gemini-1.5-pro e ver como ele se desempenha!\n\nFaça essa mudança e depois execute este trecho de código!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import evaluate, Client\nfrom langsmith.schemas import Example, Run\n\ndef target_function(inputs: dict):\n    return langsmith_rag(inputs[\"question\"])\n\nevaluate(\n    target_function,\n    data=dataset_name,\n    evaluators=[is_concise_enough],\n    experiment_prefix=\"gemini-1.5-pro\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Executando sobre Diferentes Pedaços de Dados"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Versão do Dataset\n\nVocê pode executar um experimento em uma versão específica de um dataset no sdk usando o parâmetro `as_of` em `list_examples`\n\nVamos tentar executar apenas em nosso dataset inicial."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "evaluate(\n    target_function,\n    data=client.list_examples(dataset_name=dataset_name, as_of=\"initial dataset\"),   # Usamos as_of para especificar uma versão\n    evaluators=[is_concise_enough],\n    experiment_prefix=\"versão do dataset inicial\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Divisão do Dataset\n\nVocê pode executar um experimento em uma divisão específica do seu dataset, vamos tentar executar na divisão Crucial Examples."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "evaluate(\n    target_function,\n    data=client.list_examples(dataset_name=dataset_name, splits=[\"Crucial Examples\"]),  # Passamos uma lista de Divisões\n    evaluators=[is_concise_enough],\n    experiment_prefix=\"divisão Crucial Examples\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Pontos de Dados Específicos\n\nVocê pode especificar pontos de dados individuais para executar um experimento também"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "evaluate(\n    target_function,\n    data=client.list_examples(\n        dataset_name=dataset_name, \n        example_ids=[   # Passamos uma lista específica de example_ids\n            # TODO: Você precisará colar seus próprios example ids para isso funcionar!\n            \"\",\n            \"\"\n        ]\n    ),\n    evaluators=[is_concise_enough],\n    experiment_prefix=\"dois example ids específicos\"\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Outros Parâmetros"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Repetições\n\nVocê pode executar um experimento várias vezes para garantir que tenha resultados consistentes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "evaluate(\n    target_function,\n    data=dataset_name,\n    evaluators=[is_concise_enough],\n    experiment_prefix=\"duas repetições\",\n    num_repetitions=2   # Este campo tem padrão 1\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Concorrência\nVocê também pode iniciar threads concorrentes de execução para fazer seus experimentos terminarem mais rápido!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "evaluate(\n    target_function,\n    data=dataset_name,\n    evaluators=[is_concise_enough],\n    experiment_prefix=\"concorrência\",\n    max_concurrency=3,  # Este campo tem padrão None, então isso é uma melhoria!\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Metadados\n\nVocê pode (e deve) adicionar metadados aos seus experimentos, para torná-los mais fáceis de encontrar na UI"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "evaluate(\n    target_function,\n    data=dataset_name,\n    evaluators=[is_concise_enough],\n    experiment_prefix=\"metadados adicionados\",\n    metadata={  # Podemos passar metadados personalizados para o experimento, como o nome do modelo\n        \"model_name\": MODEL_NAME\n    }\n)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}