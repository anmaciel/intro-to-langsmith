{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Métodos Alternativos de Rastreamento"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Até agora neste módulo, analisamos o decorador traceable e como podemos usá-lo para configurar o rastreamento.\n\nNesta lição, vamos analisar maneiras alternativas de configurar o rastreamento e quando você deve pensar em usar essas diferentes abordagens."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LangChain e LangGraph"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Se estivermos usando LangChain ou LangGraph, tudo o que precisamos fazer para configurar o rastreamento é definir algumas variáveis de ambiente"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Você pode defini-las diretamente\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # Se você não definir isso, os rastreamentos irão para o projeto Default"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Não se preocupe muito com nossa implementação de grafo aqui, você pode aprender mais sobre LangGraph através do nosso curso LangGraph Academy!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import nest_asyncio\nimport operator\nfrom langchain.schema import Document\nfrom langchain_core.messages import HumanMessage, AnyMessage, get_buffer_string\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langgraph.graph import StateGraph, START, END\nfrom IPython.display import Image, display\nfrom typing import List\nfrom typing_extensions import TypedDict, Annotated\nfrom utils import get_vector_db_retriever, RAG_PROMPT\n\nnest_asyncio.apply()\n\nretriever = get_vector_db_retriever()\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n\n# Define Graph state\nclass GraphState(TypedDict):\n    question: str\n    messages: Annotated[List[AnyMessage], operator.add]\n    documents: List[Document]\n\n# Define Nodes\ndef retrieve_documents(state: GraphState):\n    messages = state.get(\"messages\", [])\n    question = state[\"question\"]\n    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n    return {\"documents\": documents}\n\ndef generate_response(state: GraphState):\n    question = state[\"question\"]\n    messages = state[\"messages\"]\n    documents = state[\"documents\"]\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    \n    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n\n# Define Graph\ngraph_builder = StateGraph(GraphState)\ngraph_builder.add_node(\"retrieve_documents\", retrieve_documents)\ngraph_builder.add_node(\"generate_response\", generate_response)\ngraph_builder.add_edge(START, \"retrieve_documents\")\ngraph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\ngraph_builder.add_edge(\"generate_response\", END)\n\nsimple_rag_graph = graph_builder.compile()\ndisplay(Image(simple_rag_graph.get_graph().draw_mermaid_png()))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Estamos configurando um grafo simples no LangGraph. Se você quiser aprender mais sobre LangGraph, eu recomendaria fortemente dar uma olhada em nosso curso LangGraph Academy.\n\nVocê também pode passar metadados ou outros campos através de uma configuração opcional"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Como configurar rastreamento se estou usando LangChain?\"\nsimple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Vamos dar uma olhada no LangSmith!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Gerenciador de Contexto de Rastreamento"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Em Python, você pode usar o gerenciador de contexto trace para registrar rastreamentos no LangSmith. Isso é útil em situações onde:\n\nVocê quer registrar rastreamentos para um bloco específico de código.\nVocê quer controle sobre as entradas, saídas e outros atributos do rastreamento.\nNão é viável usar um decorador ou wrapper.\nQualquer um ou todos os itens acima.\nO gerenciador de contexto se integra perfeitamente com o decorador traceable e o wrapper wrap_openai, então você pode usá-los juntos na mesma aplicação.\n\nVocê ainda precisa definir seu `LANGSMITH_API_KEY` e `LANGSMITH_TRACING`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import traceable, trace\nfrom typing import List\nimport nest_asyncio\nfrom utils import get_vector_db_retriever\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\n\nMODEL_PROVIDER = \"google\"\nMODEL_NAME = \"gemini-1.5-flash\"\nAPP_VERSION = 1.0\nRAG_SYSTEM_PROMPT = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\nSe você não souber a resposta, apenas diga que não sabe.\nUse no máximo três frases e mantenha a resposta concisa.\n\"\"\"\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\n\"\"\"\nretrieve_documents\n- Retorna documentos buscados de um armazenamento vetorial baseado na pergunta do usuário\n\"\"\"\n@traceable\ndef retrieve_documents(question: str):\n    documents = retriever.invoke(question)\n    return documents\n\n\"\"\"\ngenerate_response\n- Chama `call_gemini` para gerar uma resposta do modelo após formatar entradas\n\"\"\"\n# TODO: Remove traceable, and use with trace()\n@traceable\ndef generate_response(question: str, documents):\n    # NOTA: Nossos documentos vieram como uma lista de objetos, mas queremos apenas registrar uma string\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n\n    # TODO: Use with trace()\n    # with trace(\n    #     name=\"Gerar Resposta\",\n    #     run_type=\"chain\",\n    #     inputs={\"question\": question, \"formatted_docs\": formatted_docs},\n    #     metadata={\"foo\": \"bar\"},\n    # ) as ls_trace:\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n        }\n    ]\n    response = call_gemini(messages)\n    # TODO: Finalize seu rastreamento e escreva as saídas para o LangSmith\n    # ls_trace.end(outputs={\"output\": response})\n    return response\n\n\"\"\"\ncall_gemini\n- Retorna a saída de conclusão de chat do Google Gemini\n\"\"\"\n@traceable(\n    metadata={\n        \"ls_provider\": MODEL_PROVIDER,\n        \"ls_model_name\": MODEL_NAME\n    }\n)\ndef call_gemini(\n    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n) -> str:\n    return call_gemini_chat(model, messages, temperature)\n\n\"\"\"\nlangsmith_rag\n- Chama `retrieve_documents` para buscar documentos\n- Chama `generate_response` para gerar uma resposta baseada nos documentos buscados\n- Retorna a resposta do modelo\n\"\"\"\n@traceable\ndef langsmith_rag(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Como rastreio com contexto de rastreamento?\"\nai_answer = langsmith_rag(question)\nprint(ai_answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Integrações Gemini com LangSmith"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Para modelos Google Gemini, utilizamos funções utilitárias personalizadas para integração com LangSmith, já que não há wrapper oficial equivalente ao wrap_openai. O decorador @traceable funciona perfeitamente com chamadas Gemini através de nossas funções helper.\n\nVocê ainda precisa definir seu `GOOGLE_API_KEY` e `LANGSMITH_TRACING`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import traceable\nfrom typing import List\nimport nest_asyncio\nfrom utils import get_vector_db_retriever\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\n\nMODEL_PROVIDER = \"google\"\nMODEL_NAME = \"gemini-1.5-flash\"\nAPP_VERSION = 1.0\nRAG_SYSTEM_PROMPT = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\nSe você não souber a resposta, apenas diga que não sabe.\nUse no máximo três frases e mantenha a resposta concisa.\n\"\"\"\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\n@traceable(run_type=\"chain\")\ndef retrieve_documents(question: str):\n    return retriever.invoke(question)\n\n@traceable(run_type=\"chain\")\ndef generate_response(question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n        }\n    ]\n    return call_gemini(messages)\n\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": MODEL_PROVIDER,\n        \"ls_model_name\": MODEL_NAME\n    }\n)\ndef call_gemini(\n    messages: List[dict],\n) -> str:\n    return call_gemini_chat(MODEL_NAME, messages, 0.0)\n\n@traceable(run_type=\"chain\")\ndef langsmith_rag_with_gemini(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Como rastreio com Google Gemini?\"\nai_answer = langsmith_rag_with_gemini(question)\nprint(ai_answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "As funções Gemini com @traceable aceitas parâmetros langsmith_extra similares às funções decoradas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Qual é a cor do céu?\"\nai_answer = langsmith_rag_with_gemini(\n    question, \n    langsmith_extra={\"metadata\": {\"foo\": \"bar\"}}\n)\nprint(ai_answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": "## [Avançado] RunTree"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Outra maneira mais explícita de registrar rastreamentos no LangSmith é através da API RunTree. Esta API permite mais controle sobre seu rastreamento - você pode criar manualmente execuções e execuções filhas para montar seu rastreamento. Você ainda precisa definir seu `LANGSMITH_API_KEY`, mas `LANGSMITH_TRACING` não é necessário para este método."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from dotenv import load_dotenv\n# Tenho minhas variáveis de ambiente definidas em um arquivo .env\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Vamos definir `LANGSMITH_TRACING` como false, pois estamos usando RunTree para criar execuções manualmente neste caso."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ[\"LANGSMITH_TRACING\"] = \"false\"\n\nfrom langsmith import utils\nutils.tracing_is_enabled() # Isso deve retornar false"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Reescrevemos nossa aplicação RAG, exceto que desta vez passamos um argumento RunTree através de nossas chamadas de função e criamos execuções filhas em cada camada. Isso dá ao nosso RunTree a mesma hierarquia que pudemos estabelecer automaticamente com @traceable"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import RunTree\nfrom typing import List\nimport nest_asyncio\nfrom utils import get_vector_db_retriever\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\ndef retrieve_documents(parent_run: RunTree, question: str):\n    # Criar uma execução filha\n    child_run = parent_run.create_child(\n        name=\"Recuperar Documentos\",\n        run_type=\"retriever\",\n        inputs={\"question\": question},\n    )\n    documents = retriever.invoke(question)\n    # Postar a saída de nossa execução filha\n    child_run.end(outputs={\"documents\": documents})\n    child_run.post()\n    return documents\n\ndef generate_response(parent_run: RunTree, question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    rag_system_prompt = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\n    Use as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\n    Se você não souber a resposta, apenas diga que não sabe.\n    Use no máximo três frases e mantenha a resposta concisa.\n    \"\"\"\n    # Criar uma execução filha\n    child_run = parent_run.create_child(\n        name=\"Gerar Resposta\",\n        run_type=\"chain\",\n        inputs={\"question\": question, \"documents\": documents},\n    )\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": rag_system_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n        }\n    ]\n    gemini_response = call_gemini(child_run, messages)\n    # Postar a saída de nossa execução filha\n    child_run.end(outputs={\"gemini_response\": gemini_response})\n    child_run.post()\n    return gemini_response\n\ndef call_gemini(\n    parent_run: RunTree, messages: List[dict], model: str = \"gemini-1.5-flash\", temperature: float = 0.0\n) -> str:\n    # Criar uma execução filha\n    child_run = parent_run.create_child(\n        name=\"Chamada Gemini\",\n        run_type=\"llm\",\n        inputs={\"messages\": messages},\n        metadata={\n            \"ls_provider\": \"google\",\n            \"ls_model_name\": model\n        }\n    )\n    gemini_response = call_gemini_chat(model, messages, temperature)\n    # Postar a saída de nossa execução filha\n    child_run.end(outputs={\"gemini_response\": gemini_response})\n    child_run.post()\n    return gemini_response\n\ndef langsmith_rag(question: str):\n    # Criar um RunTree raiz\n    root_run_tree = RunTree(\n        name=\"Pipeline de Chat\",\n        run_type=\"chain\",\n        inputs={\"question\": question}\n    )\n\n    # Passar nosso RunTree para as chamadas de função aninhadas\n    documents = retrieve_documents(root_run_tree, question)\n    response = generate_response(root_run_tree, question, documents)\n    output = response\n\n    # Postar nossa saída final\n    root_run_tree.end(outputs={\"generation\": output})\n    root_run_tree.post()\n    return output\n    "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Como posso rastrear com RunTree?\"\nai_answer = langsmith_rag(question)\nprint(ai_answer)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}