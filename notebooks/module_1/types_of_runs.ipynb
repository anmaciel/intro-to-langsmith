{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Rastreamento para Diferentes Tipos de Execuções"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Tipos de Execuções"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "LangSmith suporta muitos tipos diferentes de Execuções - você pode especificar que tipo sua Execução é no decorador @traceable. Os tipos de execuções são:\n\n- LLM: Invoca um LLM\n- Retriever: Recupera documentos de bancos de dados ou outras fontes\n- Tool: Executa ações com chamadas de função\n- Chain: Tipo padrão; combina múltiplas Execuções em um processo maior\n- Prompt: Hidrata um prompt para ser usado com um LLM\n- Parser: Extrai dados estruturados"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configuração"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Você pode defini-las diretamente!\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Execuções LLM para Modelos de Chat"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "LangSmith fornece renderização e processamento especiais para rastreamentos LLM. Para aproveitar ao máximo este recurso, você deve registrar seus rastreamentos LLM em um formato específico."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Para modelos estilo chat, as entradas devem ser uma lista de mensagens em formato compatível com OpenAI, representadas como dicionários Python ou objetos TypeScript. Cada mensagem deve conter as chaves role e content.\n\nA saída é aceita em qualquer um dos seguintes formatos:\n\n- Um dicionário/objeto que contém a chave choices com um valor que é uma lista de dicionários/objetos. Cada dicionário/objeto deve conter a chave message, que mapeia para um objeto de mensagem com as chaves role e content.\n- Um dicionário/objeto que contém a chave message com um valor que é um objeto de mensagem com as chaves role e content.\n- Uma tupla/array de dois elementos, onde o primeiro elemento é o role e o segundo elemento é o content.\n- Um dicionário/objeto que contém as chaves role e content.\n\nA entrada para sua função deve ser nomeada messages.\n\nVocê também pode fornecer os seguintes campos de metadados para ajudar o LangSmith a identificar o modelo e calcular custos. Se usar LangChain ou funções helper Gemini, esses campos serão preenchidos automaticamente corretamente.\n- ls_provider: O provedor do modelo, ex. \"google\", \"anthropic\", etc.\n- ls_model_name: O nome do modelo, ex. \"gemini-1.5-flash\", \"claude-3-opus-20240307\", etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import traceable\n\ninputs = [\n  {\"role\": \"system\", \"content\": \"Você é um assistente útil.\"},\n  {\"role\": \"user\", \"content\": \"Gostaria de reservar uma mesa para dois.\"},\n]\n\noutput = {\n  \"choices\": [\n      {\n          \"message\": {\n              \"role\": \"assistant\",\n              \"content\": \"Claro, que horas você gostaria de reservar a mesa?\"\n          }\n      }\n  ]\n}\n\n# Também pode usar um dos:\n# output = {\n#     \"message\": {\n#         \"role\": \"assistant\",\n#         \"content\": \"Claro, que horas você gostaria de reservar a mesa?\"\n#     }\n# }\n#\n# output = {\n#     \"role\": \"assistant\",\n#     \"content\": \"Claro, que horas você gostaria de reservar a mesa?\"\n# }\n#\n# output = [\"assistant\", \"Claro, que horas você gostaria de reservar a mesa?\"]\n\n@traceable(\n  run_type=\"llm\",\n  metadata={\n      \"ls_provider\": \"google\",\n      \"ls_model_name\": \"gemini-1.5-flash\"\n  }\n)\ndef chat_model(messages: list):\n  return output\n\nchat_model(inputs)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Lidando com Execuções LLM de Streaming"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Para streaming, você pode \"reduzir\" as saídas para o mesmo formato da versão não-streaming. Isso é atualmente suportado apenas em Python."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def _reduce_chunks(chunks: list):\n    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n\n@traceable(\n    run_type=\"llm\",\n    metadata={\"ls_provider\": \"google\", \"ls_model_name\": \"gemini-1.5-flash\"},\n    reduce_fn=_reduce_chunks\n)\ndef my_streaming_chat_model(messages: list):\n    for chunk in [\"Olá, \" + messages[1][\"content\"]]:\n        yield {\n            \"choices\": [\n                {\n                    \"message\": {\n                        \"content\": chunk,\n                        \"role\": \"assistant\",\n                    }\n                }\n            ]\n        }\n\nlist(\n    my_streaming_chat_model(\n        [\n            {\"role\": \"system\", \"content\": \"Você é um assistente útil. Por favor, cumprimente o usuário.\"},\n            {\"role\": \"user\", \"content\": \"polly o papagaio\"},\n        ],\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Execuções de Retriever + Documentos"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Muitas aplicações LLM requerem buscar documentos de bancos de dados vetoriais, grafos de conhecimento ou outros tipos de índices. Rastreamentos de retriever são uma maneira de registrar os documentos que são recuperados pelo retriever. LangSmith fornece renderização especial para etapas de recuperação em rastreamentos para facilitar o entendimento e diagnóstico de problemas de recuperação. Para que as etapas de recuperação sejam renderizadas corretamente, alguns pequenos passos precisam ser dados.\n\n1. Anote a etapa do retriever com run_type=\"retriever\".\n2. Retorne uma lista de dicionários Python ou objetos TypeScript da etapa do retriever. Cada dicionário deve conter as seguintes chaves:\n    - page_content: O texto do documento.\n    - type: Isso deve sempre ser \"Document\".\n    - metadata: Um dicionário python ou objeto TypeScript contendo metadados sobre o documento. Esses metadados serão exibidos no rastreamento."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import traceable\n\ndef _convert_docs(results):\n  return [\n      {\n          \"page_content\": r,\n          \"type\": \"Document\",\n          \"metadata\": {\"foo\": \"bar\"}\n      }\n      for r in results\n  ]\n\n@traceable(\n    run_type=\"retriever\"\n)\ndef retrieve_docs(query):\n  # Retriever retornando documentos fictícios hardcoded.\n  # Em produção, isso poderia ser um banco de dados vetorial real ou outro índice de documentos.\n  contents = [\"Conteúdo do documento 1\", \"Conteúdo do documento 2\", \"Conteúdo do documento 3\"]\n  return _convert_docs(contents)\n\nretrieve_docs(\"Consulta do usuário\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Chamada de Ferramentas"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "LangSmith tem renderização personalizada para Chamadas de Ferramentas feitas pelo modelo para deixar claro quando ferramentas fornecidas estão sendo usadas."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import traceable\nfrom typing import List, Optional\nimport json\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_with_tools\n\n@traceable(\n  run_type=\"tool\"\n)\ndef get_current_temperature(location: str, unit: str):\n    return 65 if unit == \"Fahrenheit\" else 17\n\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": \"google\",\n        \"ls_model_name\": \"gemini-1.5-flash\"\n    }\n)\ndef call_gemini(\n    messages: List[dict], tools: Optional[List[dict]]\n) -> str:\n  return call_gemini_with_tools(\"gemini-1.5-flash\", messages, tools, 0)\n\n@traceable(run_type=\"chain\")\ndef ask_about_the_weather(inputs, tools):\n  response = call_gemini(inputs, tools)\n  # Para Gemini, a resposta já contém a função chamada e resultado\n  return response\n\ntools = [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_temperature\",\n        \"description\": \"Obter a temperatura atual para um local específico\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"A cidade e estado, ex., São Paulo, SP\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\"Celsius\", \"Fahrenheit\"],\n              \"description\": \"A unidade de temperatura a usar. Infira isso da localização do usuário.\"\n            }\n          },\n          \"required\": [\"location\", \"unit\"]\n        }\n      }\n    }\n]\ninputs = [\n  {\"role\": \"system\", \"content\": \"Você é um assistente útil.\"},\n  {\"role\": \"user\", \"content\": \"Qual é o clima hoje em São Paulo?\"},\n]\n\nask_about_the_weather(inputs, tools)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}