{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fundamentos de Rastreamento"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configuração"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Make sure you set your environment variables, including your Google API key."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Você pode defini-las diretamente\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Rastreamento com @traceable"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "O decorador @traceable é uma maneira simples de registrar rastreamentos do SDK Python do LangSmith. Simplesmente decore qualquer função com @traceable.\n\nO decorador funciona criando uma árvore de execução para você cada vez que a função é chamada e inserindo-a dentro do rastreamento atual. As entradas da função, nome e outras informações são então transmitidas para o LangSmith. Se a função gerar um erro ou retornar uma resposta, essa informação também é adicionada à árvore, e as atualizações são aplicadas ao LangSmith para que você possa detectar e diagnosticar fontes de erros. Tudo isso é feito em uma thread em segundo plano para evitar bloquear a execução do seu aplicativo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Importar traceable\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\nfrom typing import List\nimport nest_asyncio\nfrom utils import get_vector_db_retriever\n\nMODEL_PROVIDER = \"google\"\nMODEL_NAME = \"gemini-1.5-flash\"\nAPP_VERSION = 1.0\nRAG_SYSTEM_PROMPT = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\nSe você não souber a resposta, apenas diga que não sabe.\nUse no máximo três frases e mantenha a resposta concisa.\n\"\"\"\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\n# TODO: Configurar rastreamento para cada função\ndef retrieve_documents(question: str):\n    return retriever.invoke(question)   # NOTA: Este é um retriever de banco vetorial LangChain, então esta chamada .invoke() será rastreada automaticamente\n\n\ndef generate_response(question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Contexto: {formatted_docs} \\n\\n Pergunta: {question}\"\n        }\n    ]\n    return call_gemini(messages)\n\n\ndef call_gemini(\n    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n) -> str:\n    return call_gemini_chat(model, messages, temperature)\n\n\ndef langsmith_rag(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "@traceable cuida do ciclo de vida do RunTree para você!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Como posso rastrear com o decorador @traceable?\"\nai_answer = langsmith_rag(question)\nprint(ai_answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Vamos dar uma olhada no LangSmith!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Adicionando Metadados"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "O LangSmith suporta o envio de metadados arbitrários junto com rastreamentos.\n\nMetadados são uma coleção de pares chave-valor que podem ser anexados a execuções. Metadados podem ser usados para armazenar informações adicionais sobre uma execução, como a versão da aplicação que gerou a execução, o ambiente no qual a execução foi gerada, ou qualquer outra informação que você queira associar a uma execução. Similar às tags, você pode usar metadados para filtrar execuções na interface do LangSmith, e podem ser usados para agrupar execuções para análise."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import traceable\n\n@traceable(\n    # TODO: Adicionar Metadados\n    # metadata={\"vectordb\": \"sklearn\"}\n)\ndef retrieve_documents(question: str):\n    return retriever.invoke(question)\n\n@traceable\ndef generate_response(question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Contexto: {formatted_docs} \\n\\n Pergunta: {question}\"\n        }\n    ]\n    return call_gemini(messages)\n\n@traceable(\n    # TODO: Adicionar Metadados\n    # metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER}\n)\ndef call_gemini(\n    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n) -> str:\n    return call_gemini_chat(model, messages, temperature)\n\n@traceable\ndef langsmith_rag(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Como adiciono Metadados a uma Execução com @traceable?\"\nai_answer = langsmith_rag(question)\nprint(ai_answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Você também pode adicionar metadados em tempo de execução!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Como adiciono metadados em tempo de execução?\"\nai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\nprint(ai_answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Vamos dar uma olhada no LangSmith!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}