{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Ciclo de Vida da Engenharia de Prompts"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configura√ß√£o"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Voc√™ pode defini-las diretamente\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou voc√™ pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Registrar um rastreamento"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from app import langsmith_rag\n\nquestion = \"Como configurar rastreamento para LangSmith com @traceable?\"\nlangsmith_rag(question)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Criar um Dataset"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Vamos criar um dataset para avaliar esta etapa espec√≠fica de nossa aplica√ß√£o"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import Client\n\nexample_dataset = [\n    (\n        \"Como configurar rastreamento para LangSmith com @traceable?\",\n        \"\"\" 2. Registrar um rastreamento‚Äã\\nUma vez que voc√™ configurou seu ambiente, pode chamar execut√°veis do LangChain normalmente.\\nLangSmith inferir√° a configura√ß√£o de rastreamento adequada:\\n\\nComo registrar e visualizar rastreamentos no LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n2. Registrar um rastreamento‚Äã\\nUma vez que voc√™ configurou seu ambiente, envolva ou decore as fun√ß√µes/SDKs personalizados que voc√™ quer rastrear.\\nLangSmith ent√£o inferir√° a configura√ß√£o de rastreamento adequada:\\n\\nIr para conte√∫do principalIr para Docs da APIBuscarRegi√£oUSEUIr para AppIn√≠cio r√°pidoObservabilidadeGuia ConceitualGuias Como FazerRastreamentoAnotar c√≥digo para rastreamentoAtivar e desativar rastreamentoFazer upload de arquivos com rastreamentosRegistrar rastreamentos para projeto espec√≠ficoDefinir taxa de amostragem para rastreamentosAdicionar metadados e tags aos rastreamentosImplementar rastreamento distribu√≠doAcessar a execu√ß√£o atual (span) dentro de uma fun√ß√£o rastreadaRegistrar rastreamentos multimodaisRegistrar rastreamentos de retrieverRegistrar rastreamentos LLM personalizadosEvitar registro de dados sens√≠veis em rastreamentosConsultar rastreamentosCompartilhar ou descompartilhar um rastreamento publicamenteComparar rastreamentosRastrear fun√ß√µes geradorasRastrear com LangChain (Python e JS/TS)Rastrear com LangGraph (Python e JS/TS)Rastrear com Instructor (apenas Python)Rastrear com Vercel AI SDK (apenas JS/TS)Rastrear sem definir vari√°veis de ambienteRastrear usando a API REST do LangSmithCalcular custos baseados em tokens para rastreamentosResolu√ß√£o de problemas de aninhamento de rastreamento[Beta] Exporta√ß√£o em lote de dados de rastreamentoRastrear fun√ß√µes JS em ambientes serverlessMonitoramento e automa√ß√µesTutoriaisAdicionar observabilidade √† sua aplica√ß√£o LLMAvalia√ß√£oEngenharia de PromptsImplanta√ß√£o (LangGraph Cloud)Administra√ß√£oAuto-hospedagemRefer√™nciaObservabilidadeGuias Como FazerRastreamentoAnotar c√≥digo para rastreamentoNesta p√°ginaAnotar c√≥digo para rastreamento\\nExistem v√°rias maneiras de registrar rastreamentos no LangSmith.\\ndicaSe voc√™ est√° usando LangChain (Python ou JS/TS), pode pular esta se√ß√£o e ir diretamente para as instru√ß√µes espec√≠ficas do LangChain.\\nUsar @traceable / traceable‚Äã\\nLangSmith facilita o registro de rastreamentos com mudan√ßas m√≠nimas ao seu c√≥digo existente com o decorador @traceable em Python e fun√ß√£o traceable em TypeScript.\\nnotaA vari√°vel de ambiente LANGSMITH_TRACING deve estar definida como 'true' para que os rastreamentos sejam registrados no LangSmith, mesmo ao usar @traceable ou traceable. Isso permite ativar e desativar o rastreamento sem mudar seu c√≥digo.Al√©m disso, voc√™ precisar√° definir a vari√°vel de ambiente LANGSMITH_API_KEY com sua chave de API (veja Configura√ß√£o para mais informa√ß√µes).Por padr√£o, os rastreamentos ser√£o registrados em um projeto chamado default.\\nPara registrar rastreamentos em um projeto diferente, veja esta se√ß√£o. \\n\\n\"\"\",\n        \"Para configurar rastreamento no LangSmith usando o decorador `@traceable` em Python, primeiro certifique-se de ter a vari√°vel de ambiente `LANGSMITH_TRACING` definida como 'true' e a `LANGSMITH_API_KEY` definida com sua chave de API. Ent√£o, voc√™ pode simplesmente decorar suas fun√ß√µes assim:\\n\\n```python\\nfrom langsmith import traceable\\n\\n@traceable\\ndef minha_funcao():\\n    # Seu c√≥digo aqui\\n    pass\\n```\\n\\nIsso registrar√° rastreamentos para `minha_funcao` automaticamente uma vez que as vari√°veis de ambiente necess√°rias estejam configuradas.\"\n    ),\n    (\n        \"Como posso usar o gerenciador de contexto de rastreamento?\",\n        \"\"\"Usar o gerenciador de contexto trace (apenas Python)‚Äã\\nEm Python, voc√™ pode usar o gerenciador de contexto trace para registrar rastreamentos no LangSmith. Isso √© √∫til em situa√ß√µes onde:\\n\\nVoc√™ quer registrar rastreamentos para um bloco espec√≠fico de c√≥digo.\\nVoc√™ quer controle sobre as entradas, sa√≠das e outros atributos do rastreamento.\\nN√£o √© vi√°vel usar um decorador ou wrapper.\\nQualquer um ou todos os itens acima.\\n\\nEm alguns ambientes, n√£o √© poss√≠vel definir vari√°veis de ambiente. Nesses casos, voc√™ pode definir a configura√ß√£o de rastreamento programaticamente.\\nComportamento recentemente alteradoDevido a v√°rias solicita√ß√µes para controle mais fino do rastreamento usando o gerenciador de contexto trace,\\nmudamos o comportamento do with trace para honrar a vari√°vel de ambiente LANGSMITH_TRACING na vers√£o 0.1.95 do SDK Python. Voc√™ pode encontrar mais detalhes nas notas de lan√ßamento.\\nA maneira recomendada de desabilitar/habilitar rastreamento sem definir vari√°veis de ambiente √© usar o gerenciador de contexto with tracing_context, como mostrado no exemplo abaixo.\\nPythonTypeScriptA maneira recomendada de fazer isso em Python √© usar o gerenciador de contexto tracing_context. Isso funciona tanto para c√≥digo anotado com traceable quanto para c√≥digo dentro do gerenciador de contexto trace.\\n\\nIr para conte√∫do principalIr para Docs da APIBuscarRegi√£oUSEUIr para AppIn√≠cio r√°pidoObservabilidadeGuia ConceitualGuias Como FazerRastreamentoAnotar c√≥digo para rastreamentoAtivar e desativar rastreamentoFazer upload de arquivos com rastreamentosRegistrar rastreamentos para projeto espec√≠ficoDefinir taxa de amostragem para rastreamentosAdicionar metadados e tags aos rastreamentosImplementar rastreamento distribu√≠doAcessar a execu√ß√£o atual (span) dentro de uma fun√ß√£o rastreadaRegistrar rastreamentos multimodaisRegistrar rastreamentos de retrieverRegistrar rastreamentos LLM personalizadosEvitar registro de dados sens√≠veis em rastreamentosConsultar rastreamentosCompartilhar ou descompartilhar um rastreamento publicamenteComparar rastreamentosRastrear fun√ß√µes geradorasRastrear com LangChain (Python e JS/TS)Rastrear com LangGraph (Python e JS/TS)Rastrear com Instructor (apenas Python)Rastrear com Vercel AI SDK (apenas JS/TS)Rastrear sem definir vari√°veis de ambienteRastrear usando a API REST do LangSmithCalcular custos baseados em tokens para rastreamentosResolu√ß√£o de problemas de aninhamento de rastreamento[Beta] Exporta√ß√£o em lote de dados de rastreamentoRastrear fun√ß√µes JS em ambientes serverlessMonitoramento e automa√ß√µesTutoriaisAdicionar observabilidade √† sua aplica√ß√£o LLMAvalia√ß√£oEngenharia de PromptsImplanta√ß√£o (LangGraph Cloud)Administra√ß√£oAuto-hospedagemRefer√™nciaObservabilidadeGuias Como FazerRastreamentoResolu√ß√£o de problemas de aninhamento de rastreamentoNesta p√°ginaResolu√ß√£o de problemas de aninhamento de rastreamento\\nAo rastrear com o SDK LangSmith, LangGraph e LangChain, o rastreamento deve propagar automaticamente o contexto correto para que o c√≥digo executado dentro de um rastreamento pai seja renderizado no local esperado na UI.\\nSe voc√™ vir uma execu√ß√£o filha ir para um rastreamento separado (e aparecer no n√≠vel superior), pode ser causado por um dos seguintes 'casos extremos' conhecidos.\\nPython‚Äã\\nO seguinte descreve causas comuns para rastreamentos 'divididos' ao construir com python.\\nPropaga√ß√£o de contexto usando asyncio‚Äã\\nAo usar chamadas ass√≠ncronas (especialmente com streaming) em vers√µes do Python < 3.11, voc√™ pode encontrar problemas com aninhamento de rastreamento. Isso √© porque o asyncio do Python s√≥ adicionou suporte completo para passar contexto na vers√£o 3.11.\\nPor que‚Äã\\nLangChain e LangSmith SDK usam contextvars para propagar informa√ß√µes de rastreamento implicitamente. No Python 3.11 e acima, isso funciona perfeitamente. No entanto, em vers√µes anteriores (3.8, 3.9, 3.10), tarefas asyncio carecem de suporte adequado a contextvar, o que pode levar a rastreamentos desconectados.\\nPara resolver‚Äã\\n\\nPropaga√ß√£o de contexto usando threading‚Äã\\n√â comum come√ßar o rastreamento e querer aplicar algum paralelismo em tarefas filhas, tudo dentro de um √∫nico rastreamento. O ThreadPoolExecutor do stdlib do Python por padr√£o quebra o rastreamento.\\nPor que‚Äã\\nAs contextvars do Python come√ßam vazias dentro de novas threads. Aqui est√£o duas abordagens para lidar com a manuten√ß√£o da continuidade do rastreamento:\\nPara resolver‚Äã\\n\\n\\nUsando ContextThreadPoolExecutor do LangSmith\\nLangSmith fornece um ContextThreadPoolExecutor que automaticamente lida com propaga√ß√£o de contexto:\\nfrom langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledef outer_func():    with ContextThreadPoolExecutor() as executor:        inputs = [1, 2]        r = list(executor.map(inner_func, inputs))@traceabledef inner_func(x):    print(x)outer_func()\\n\\n\\nFornecendo manualmente a √°rvore de execu√ß√£o pai\\nAlternativamente, voc√™ pode passar manualmente a √°rvore de execu√ß√£o pai para a fun√ß√£o interna:\\nfrom langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledef outer_func():    rt = get_current_run_tree()    with ThreadPoolExecutor() as executor:        r = list(            executor.map(                lambda x: inner_func(x, langsmith_extra={\\\"parent\\\": rt}), [1, 2]            )        )@traceabledef inner_func(x):    print(x)outer_func()\\nNesta abordagem, usamos get_current_run_tree() para obter a √°rvore de execu√ß√£o atual e pass√°-la para a fun√ß√£o interna usando o par√¢metro langsmith_extra. \\n\\n \"\"\",\n        \"Voc√™ pode usar o gerenciador de contexto de rastreamento envolvendo o c√≥digo que voc√™ quer rastrear com a declara√ß√£o `with trace_context:`. Aqui est√° um exemplo simples:\\n\\n```python\\nfrom langsmith import trace_context\\n\\nwith trace_context:\\n    # Seu c√≥digo rastre√°vel aqui\\n    print(\\\"Rastreando este bloco de c√≥digo.\\\")\\n``` \\n\\nIsso registrar√° rastreamentos no LangSmith para o bloco de c√≥digo especificado.\"\n    ),\n    (\n        \"Como posso usar RunTree?\",\n        \"\"\"PythonTypeScriptimport sys\\nsys.path.append('../../')\\nfrom gemini_utils import call_gemini_chat\\nfrom langsmith.run_trees import RunTree# Isso pode ser uma entrada do usu√°rio para sua appquestion = \\\"Voc√™ pode resumir as reuni√µes desta manh√£?\\\"# Criar uma execu√ß√£o de n√≠vel superiort√≠peline = RunTree(  name=\\\"Pipeline de Chat\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})# Isso pode ser recuperado em uma etapa de recupera√ß√£ocontext = \\\"Durante a reuni√£o desta manh√£, resolvemos todos os conflitos mundiais.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"Voc√™ √© um assistente √∫til. Por favor, responda √† solicita√ß√£o do usu√°rio apenas com base no contexto fornecido.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Pergunta: {question}\\\\nContexto: {context}\\\"}]# Criar uma execu√ß√£o filhachild_llm_run = pipeline.create_child(  name=\\\"Chamada Gemini\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Gerar uma conclus√£ochat_completion = call_gemini_chat(\\\"gemini-1.5-flash\\\", messages, 0.0)# Finalizar as execu√ß√µes e registr√°-laschild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion})pipeline.postRun()import { ChatGoogleGenerativeAI } from \\\"@langchain/google-genai\\\";import { RunTree } from \\\"langsmith\\\";// Isso pode ser uma entrada do usu√°rio para sua appconst question = \\\"Voc√™ pode resumir as reuni√µes desta manh√£?\\\";const pipeline = new RunTree({  name: \\\"Pipeline de Chat\\\",  run_type: \\\"chain\\\",  inputs: { question }});await pipeline.postRun();// Isso pode ser recuperado em uma etapa de recupera√ß√£oconst context = \\\"Durante a reuni√£o desta manh√£, resolvemos todos os conflitos mundiais.\\\";const messages = [  { role: \\\"system\\\", content: \\\"Voc√™ √© um assistente √∫til. Por favor, responda √† solicita√ß√£o do usu√°rio apenas com base no contexto fornecido.\\\" },  { role: \\\"user\\\", content: `Pergunta: ${question}Contexto: ${context}` }];// Criar uma execu√ß√£o filhaconst childRun = await pipeline.createChild({  name: \\\"Chamada Gemini\\\",\\n\\nAlternativamente, voc√™ pode converter o RunnableConfig do LangChain para um objeto RunTree equivalente usando RunTree.fromRunnableConfig ou passar o RunnableConfig como o primeiro argumento da fun√ß√£o envolvida por traceable.\\n\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser\\nprompt = ChatPromptTemplate.from_messages([(\\\"system\\\", \\\"Voc√™ √© uma IA √∫til.\\\"),(\\\"user\\\", \\\"{input}\\\")])\\nchat_model = ChatGoogleGenerativeAI(model=\\\"gemini-1.5-flash\\\")\\noutput_parser = StrOutputParser()\\n# Tags e metadados podem ser configurados com RunnableConfig\\nchain = (prompt | chat_model | output_parser).with_config({\\\"tags\\\": [\\\"tag-de-n√≠vel-superior\\\"], \\\"metadata\\\": {\\\"chave-de-n√≠vel-superior\\\": \\\"valor-de-n√≠vel-superior\\\"}})\\n# Tags e metadados tamb√©m podem ser passados em tempo de execu√ß√£o\\nchain.invoke({\\\"input\\\": \\\"Qual √© o sentido da vida?\\\"}, {\\\"tags\\\": [\\\"tags-compartilhadas\\\"], \\\"metadata\\\": {\\\"chave-compartilhada\\\": \\\"valor-compartilhado\\\"}})\\nimport { ChatGoogleGenerativeAI } from \\\"@langchain/google-genai\\\";\\nimport { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";\\nimport { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";\\nconst prompt = ChatPromptTemplate.fromMessages([[\\\"system\\\", \\\"Voc√™ √© uma IA √∫til.\\\"],[\\\"user\\\", \\\"{input}\\\"])\\nconst model = new ChatGoogleGenerativeAI({ modelName: \\\"gemini-1.5-flash\\\" });\\nconst outputParser = new StringOutputParser();\\n// Tags e metadados podem ser configurados com RunnableConfig\\nconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\\\"tags\\\": [\\\"tag-de-n√≠vel-superior\\\"], \\\"metadata\\\": {\\\"chave-de-n√≠vel-superior\\\": \\\"valor-de-n√≠vel-superior\\\"}});\\n// Tags e metadados tamb√©m podem ser passados em tempo de execu√ß√£o\\n\\n\"\"\",\n        \"Voc√™ pode usar `RunTree` criando um pipeline onde voc√™ define seu nome, tipo e entradas. Por exemplo:\\n\\n```python\\nfrom langsmith.run_trees import RunTree\\n\\npipeline = RunTree(name=\\\"Meu Pipeline\\\", run_type=\\\"chain\\\", inputs={\\\"question\\\": \\\"Sua pergunta aqui\\\"})\\n```\\n\\nVoc√™ pode ent√£o criar execu√ß√µes filhas e gerenci√°-las adequadamente dentro do pipeline.\"\n    ),\n]\n\nclient = Client()\ndataset_name = \"Perguntas T√©cnicas\"\n\n# Criar dataset\ndataset = client.create_dataset(\n    dataset_name=dataset_name, description=\"Perguntas t√©cnicas sobre LangSmith\"\n)\n\n# Preparar entradas e sa√≠das\ninputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\noutputs = [{\"output\": o} for _, _, o in example_dataset]\n\n# Criar exemplos no dataset\nclient.create_examples(\n    inputs=inputs,\n    outputs=outputs,\n    dataset_id=dataset.id,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Atualizar nossa Aplica√ß√£o para usar Prompt Hub"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Vamos definir praticamente a mesma aplica√ß√£o RAG de antes - com uma melhoria crucial.\n\nEm vez de puxar nosso `RAG_PROMPT` do utils.py, vamos nos conectar ao Prompt Hub no LangSmith.\n\nVamos adicionar o trecho de c√≥digo que puxar√° nosso prompt no qual acabamos de iterar!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport tempfile\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders.sitemap import SitemapLoader\nfrom langchain_community.vectorstores import SKLearnVectorStore\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langsmith import traceable\nfrom langsmith.client import convert_prompt_to_openai_format\nimport sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\nfrom typing import List\nimport nest_asyncio\n\nMODEL_NAME = \"gemini-1.5-flash\"\nMODEL_PROVIDER = \"google\"\nAPP_VERSION = 1.0\n\n# TODO: Remover este prompt hard-coded e substitu√≠-lo por Prompt Hub\nRAG_SYSTEM_PROMPT = \"\"\"Voc√™ √© um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder √† pergunta mais recente na conversa.\nSe voc√™ n√£o souber a resposta, apenas diga que n√£o sabe.\nUse no m√°ximo tr√™s frases e mantenha a resposta concisa.\n\"\"\"\n\ndef get_vector_db_retriever():\n    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n    embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n\n    # Se o armazenamento vetorial existir, ent√£o carreg√°-lo\n    if os.path.exists(persist_path):\n        vectorstore = SKLearnVectorStore(\n            embedding=embd,\n            persist_path=persist_path,\n            serializer=\"parquet\"\n        )\n        return vectorstore.as_retriever(lambda_mult=0)\n\n    # Caso contr√°rio, indexar documentos LangSmith e criar novo armazenamento vetorial\n    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n    ls_docs = ls_docs_sitemap_loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n        chunk_size=500, chunk_overlap=0\n    )\n    doc_splits = text_splitter.split_documents(ls_docs)\n\n    vectorstore = SKLearnVectorStore.from_documents(\n        documents=doc_splits,\n        embedding=embd,\n        persist_path=persist_path,\n        serializer=\"parquet\"\n    )\n    vectorstore.persist()\n    return vectorstore.as_retriever(lambda_mult=0)\n\nnest_asyncio.apply()\nretriever = get_vector_db_retriever()\n\n\"\"\"\nretrieve_documents\n- Retorna documentos buscados de um armazenamento vetorial baseado na pergunta do usu√°rio\n\"\"\"\n@traceable(run_type=\"chain\")\ndef retrieve_documents(question: str):\n    return retriever.invoke(question)\n\n\"\"\"\ngenerate_response\n- Chama `call_gemini` para gerar uma resposta do modelo ap√≥s formatar entradas\n\"\"\"\n@traceable(run_type=\"chain\")\ndef generate_response(question: str, documents):\n    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n    # TODO: Vamos usar nosso prompt puxado do Prompt Hub em vez de formatar manualmente aqui!\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": RAG_SYSTEM_PROMPT\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Contexto: {formatted_docs} \\n\\n Pergunta: {question}\"\n        }\n    ]\n    # formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n    # messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n    return call_gemini(messages)\n\n\"\"\"\ncall_gemini\n- Retorna a sa√≠da de conclus√£o de chat do Google Gemini\n\"\"\"\n@traceable(\n    run_type=\"llm\",\n    metadata={\n        \"ls_provider\": MODEL_PROVIDER,\n        \"ls_model_name\": MODEL_NAME\n    }\n)\ndef call_gemini(messages: List[dict]) -> str:\n    return call_gemini_chat(MODEL_NAME, messages, 0.0)\n\n\"\"\"\nlangsmith_rag\n- Chama `retrieve_documents` para buscar documentos\n- Chama `generate_response` para gerar uma resposta baseada nos documentos buscados\n- Retorna a resposta do modelo\n\"\"\"\n@traceable(run_type=\"chain\")\ndef langsmith_rag(question: str):\n    documents = retrieve_documents(question)\n    response = generate_response(question, documents)\n    return response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "question = \"Como configurar rastreamento para LangSmith com @traceable?\"\nlangsmith_rag(question)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}