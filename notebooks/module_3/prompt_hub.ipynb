{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Conectando ao Prompt Hub"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Podemos conectar nossa aplicação ao Prompt Hub do LangSmith, que nos permitirá testar e iterar nossos prompts dentro do LangSmith, e puxar nossas melhorias diretamente para nossa aplicação."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Configuração"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ[\"GOOGLE_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"\"\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # Se você não definir isso, os rastreamentos irão para o projeto Default"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ou você pode usar um arquivo .env\nfrom dotenv import load_dotenv\nload_dotenv(dotenv_path=\"../../.env\", override=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Puxar um prompt do Prompt Hub"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Puxe um prompt do Prompt Hub colando o trecho de código da UI."
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Vamos ver o que puxamos - note que não obtivemos o modelo, então isso é apenas um StructuredPrompt e não é executável."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Legal! Agora vamos hidratar nosso prompt chamando .invoke() com nossas entradas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "hydrated_prompt = prompt.invoke({\"question\": \"Você já é capitão?\", \"language\": \"Espanhol\"})\nhydrated_prompt"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "E agora vamos passar essas mensagens para o Gemini e ver o que obtemos de volta!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\nfrom langsmith.client import convert_prompt_to_openai_format\n\n# NOTA: Convertemos o prompt hidratado para o formato OpenAI compatível e depois para Gemini\nconverted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n\ncall_gemini_chat(\"gemini-1.5-flash\", converted_messages, 0.0)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### [Extra: Apenas LangChain] Puxando a Configuração do Modelo\n\nTambém podemos puxar a configuração do modelo salva como um LangChain RunnableBinding quando usamos `include_model=True`. Isso nos permite executar nosso template de prompt diretamente com a configuração do modelo salva."
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Teste seu prompt!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "prompt.invoke({\"question\": \"Você já é capitão?\", \"language\": \"Espanhol\"})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Puxar um commit específico"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Puxe um commit específico do Prompt Hub colando o trecho de código da UI."
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Execute este commit!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../../')\nfrom gemini_utils import call_gemini_chat\nfrom langsmith.client import convert_prompt_to_openai_format\n\nhydrated_prompt = prompt.invoke({\"question\": \"Como é o mundo?\", \"language\": \"Português\"})\n# NOTA: Podemos usar este utilitário do LangSmith para converter nosso prompt hidratado para o formato openai\nconverted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n\ncall_gemini_chat(\"gemini-1.5-flash\", converted_messages, 0.0)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Fazendo Upload de Prompts"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Você também pode facilmente atualizar seus prompts no hub programaticamente."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain.prompts.chat import ChatPromptTemplate\nfrom langsmith import Client\n\nclient=Client()\n\nfrench_prompt = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\n\nSeus usuários só falam francês, certifique-se de responder aos seus usuários apenas em francês.\n\nConversa: {conversation}\nContexto: {context} \nPergunta: {question}\nResposta:\"\"\"\n\nfrench_prompt_template = ChatPromptTemplate.from_template(french_prompt)\nclient.push_prompt(\"french-rag-prompt\", object=french_prompt_template)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Você também pode fazer push de um prompt como uma RunnableSequence de um prompt e um modelo. Isso é útil para armazenar a configuração do modelo que você quer usar com este prompt. O provedor deve ser suportado pelo playground do LangSmith."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain.prompts.chat import ChatPromptTemplate\nfrom langsmith import Client\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\nclient=Client()\nmodel = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n\nfrench_prompt = \"\"\"Você é um assistente para tarefas de perguntas e respostas.\nUse as seguintes partes do contexto recuperado para responder à pergunta mais recente na conversa.\n\nSeus usuários só falam francês, certifique-se de responder aos seus usuários apenas em francês.\n\nConversa: {conversation}\nContexto: {context} \nPergunta: {question}\nResposta:\"\"\"\nfrench_prompt_template = ChatPromptTemplate.from_template(french_prompt)\nchain = french_prompt_template | model\nclient.push_prompt(\"french-runnable-sequence\", object=chain)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}